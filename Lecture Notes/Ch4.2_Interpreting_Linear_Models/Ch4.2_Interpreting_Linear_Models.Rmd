---
title: "SML201 Chapter 4.2"
subtitle:  'Interpreting Linear Regression Models'
author: "Daisy Huang"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
  revealjs::revealjs_presentation:
    center: yes
    font: 10
    highlight: haddock
    includes:
      before_body: ../../doc_prefix.html
    left: yes
    transition: slide
editor_options:
  chunk_output_type: console
---



<!-----
---
title: "SML201 Chapter 4.2"
subtitle:  'Interpreting Linear Regression Models'
author: "Daisy Huang"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 4
  pdf_document:
    fig_caption: yes
    toc: yes
    toc_depth: 4
geometry: margin=1.5in
editor_options: 
  chunk_output_type: console
---

---------->


```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=5.5, fig.width=6, collapse=TRUE, comment="", prompt=F, echo = TRUE, cache=TRUE, autodep=TRUE, cache.comments=FALSE,
                      tidy=TRUE, tidy.opts=list(width.cutoff=60))
options(width=63)
```



# R functions covered

---------

* To fit linear models: `lm(formula, data,...)`
* To make matrix plots:  `ggpairs(data, upper = list(continuous = wrap(...)), lower = list(continuous = wrap(...))) + ...`
Graph with `ggplot` package:  
* To add a regression line to an existing scatterplot: `ggplot(...) + ... + geom_smooth(method = "lm", se=F, ...)`
* To add text strings on an existing plot: `ggplot(...) + ... + geom_text(aes(label=paste(...)), x, y, color, ... )`



# Objectives

-------------

* Being able to interpret different Linear Regression models
* Being able to choose the better models based on the exploratory analysis of the data

# Introduction

---------

In last chapter we study simple linear regression, the simplest linear regression model with just one independent variable.  

In this chapter we will study the more general form of linear models: multiple linear regression.  In multiple linear regression models we allow the model to have two or more independent variables.



# Our ultimate goal: build linear models

---------

Linear modeling has wide range of applications; e.g.,

* Predicting sold price of a house:
https://www.redfin.com/NJ/Princeton/14-Governors-Ln-08540/home/36693866

* Forecasting the medical expenses of health insurance customers


---------

Near the end of the semester you will learn in details how to build models similar to the ones mentioned in the previous slide.  In this unit, we will show you the interpretations on different models and why certain models are better than others for the data you have.  This will prepare you for model building later.



# Assessing a model

---------

Recall that MSE is often used to evaluate **how good** a numerical predictor is:

**MSE = avg((actual value - predicted value)^2)**

This means that given two models, the model that gives prediction line(s) that have the points clustered more tightly around the line(s) is the better model.

---------

We will demonstrate the concepts with the `insurance` and the `iris` datasets.

```{r}
library(ggplot2)
library(GGally)
```


-----------

Let's read in the dataset.

```{r tidy = F}
ins = read.csv(file = "/Users/billhaarlow/Desktop/SML201/Ch4.2_Interpreting_Linear_Models/insurance.csv")
```

(The dataset was downloaded from Kaggle.com (https://www.kaggle.com/mirichoi0218/insurance).)



# Variable definitions

---------

* **age**: The age of the primary beneficiary (excluding those above 64 years since they are generally covered by the government);  
* **sex**:  The policy holder's gender; 0 for female and 1 for male;  
* **bmi**:  Body mass index.  The index provides a sense of how over- or under- weight a person is.  BMI = (weight (in kg)/height (in m))^2An ideal BMI is in the range of 18.5 to 24.9.  
* **children**:  Number of children/dependents covered by the insurance plan;  
* **smoker**:  Whether the insured smokes tobacco regularly or not: 0 for no, 1 for yes;  
* **region**:  The beneficiary's place of residence in the US, divided into four geographic regions: northeast, southeast, southwest, or northwest;  
* **charges**:  Total medical expenses charged to the plan for the calendar year.




-----------

```{r}
dim(ins)
str(ins)
summary(ins)
```

-----------

# Investigate relationships between the variables

```{r warning = F, message = F}
# `upper` is used to set options for the graphs on the upper-triangle
# `lower` is used to set options for the graphs on the lower-triangle
ggpairs(ins,
  upper = list(continuous = wrap("cor", size = 2.5)),
  lower = list(continuous = wrap("points", alpha = 0.3, size=0.1))) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Pairwise relationship between variables in the
       insurance dataset")
```

# Selecting the first variable for the model

-----------

Which of the **numeric variables** has the strongest linear relationship with `charges`?

(A.) age  
(B.) bmi  
(C.) children  

-----------

Answer: age

-----------

Let's build a model to predict the value of `charge` with `age`.

Mathematically the model is:

\[
charges_i = \beta_0 + \beta_{age} age_i + error_i
\]

-----------


```{r}
# By default the lm() function includes the y-intercept for the model
simple.mod = lm(charges ~ age, data = ins)
simple.mod
```

```{r}
summary(simple.mod)
```

-----------

Recall that the mathematical form of the model fitted above is:

\[
charges_i = \beta_0 + \beta_{age} age_i + error_i
\]


What is the fitted model?  That is, what are the coefficients that minimize the MSE (in short, the least squares coefficient estimates of the model)?

\[
\hat{\beta_0} = 3165.9
\]

\[
\hat{\beta}_{age} = 257.7
\]

Thus, the fitted model is 

\[
charges_i = 3165.9 + 257.7 age_i
\]

We can add this to the scatterplot of `charges` v.s. `age`.

```{r tidy = F}
slope = round(simple.mod$coef[2], 1)
y.int = round(simple.mod$coef[1], 1)
slope
y.int

ggplot(data = ins, mapping = aes(x = age, y=charges)) + 
  geom_point() + 
  geom_smooth(method = "lm", se=F, size = 1.5, alpha=.5) +
  geom_text(aes(label=paste("charges = ", y.int, "+", slope, "age")), 
            x=35, y=32000, color = 2, size = 6) +
  labs(x="Age (years)", y="Medical expenses charged to the plan (dollars)") 
```


------------

**Residual** is defined as the (observed value - predicted value) for $Y$:  

\[
residual_i = y_i - \hat{y_i}
\]

Thus, MSE is just the average of the squared residuals:

\[
MSE = mean((residual_i)^2)
\]


Two models are compared and assessed based on how well the model fits the data.  The better model should have _______ residuals on average.

(A.) smaller  
(B.) larger

------------

## R-squared

The proportion of variation explained by the fitted model (i.e., the proportion of variation that is reduced when using the model for prediction compared to when using just the mean of the y-variable for prediction) is called $R^2$ or $r^2$.  It can be calculated as:

$$
R^2 = \frac{\Sigma_i^n (y_i - \bar{y})^2 - \Sigma_i^n(y_i - \hat{y_i})^2}{\Sigma_i^n (y_i - \bar{y})^2} 
= 1 - \frac{\Sigma_i^n((y_i - \hat{y_i})^2}{\Sigma_i^n (y_i - \bar{y})^2}
$$

-------

Note that if your model has only the intercept and with no X-variable, i.e.,

\[
charges_i = \beta_0 + error_i
\]


then, 

$$\Sigma_i^n((y_i - \hat{y_i})^2 = \Sigma_i^n (y_i - \bar{y})^2.$$

In this case, using your model does not reduce any variation compared to using just the mean of the observed y-values for prediction.  Therefore, $R^2$ is 0 in this case.  


-------


Also, note that if you add more X-variables to the model the sum of the residual squares will always either decrease or remain the same; i.e., $$\Sigma_i^n((y_i - \hat{y_i})^2$$ is always less or equal to $$\Sigma_i^n (y_i - \bar{y})^2$$.  Thus, $R^2 = 1 - \frac{\Sigma_i^n((y_i - \hat{y_i})^2}{\Sigma_i^n (y_i - \bar{y})^2}$ tells us the fraction of the variations in the $Y$-variable that is reduced by using the model. The higher the $R^2$ the better your model fits your data.


-------


**A word of caution**

$R^2$ is useful only when you include the intercept in your model.  Also, $R^2$ not a good criterion since $R^2$ always increases with model size.  Adjusted $R^2$ is better since it “penalized” bigger models.

-------

# Adding a contrast term to the model

-----------

From the matrix scatterplots which of the **categorical variables** will be the best to be added to the model?

(A.) `sex`  
(B.) `smoker`  
(C.) `region`  

If this is not clear on the first scatterplot matrix, we can also make the plot for each individual pair: `charges` and `sex`, `charges` and `smoker`, and  `charges` and `region`:

```{r}
boxplot(charges ~ sex, data = ins, main = 'Boxplots for medical charges by sex', ylab = 'medical charges', xlab = 'Gender')


boxplot(charges ~ smoker, data = ins, main = 'Boxplots for medical charges by smoking status', ylab = 'medical charges', xlab = 'smoker?', names = c("No", "Yes"))


# `las = 1` sets all axis labels always perpendicular to the axis
boxplot(charges ~ region, data = ins, main = 'Boxplots for medical charges by region', ylab = 'medical charges', las = 2, cex = .7, cex.lab = .7, cex.main = .7, xlab = 'Regions')
```

-----------

Now, let's investigate the pairwise relationship between the variables again but by smoking status.

-----------

We can also turn off (use `message = F` for the code chunk) the option of showing the output/error messages for the plot--you should always read the error message at least once to make sure that there is nothing really wrong with the procedure before turning the option of showing the message off.

```{r message = F, tidy = F}

ggpairs(ins, aes(colour = smoker, alpha = 0.4,),
  upper = list(continuous = wrap("cor", size = 1.9)),
  lower = list(continuous = wrap("points", alpha = 0.3, size=0.1))) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Pairwise relationship, 
                colored by smoking status,  
                between variables in the insurance dataset")
```


Note that the variables `sex` and `region` do not separate the values of `charges` very well; e.g., see the ggpair plot colored by `sex`:


```{r message = F, tidy = F}
ggpairs(ins, aes(colour = sex, alpha = 0.4), 
  upper = list(continuous = wrap("cor", size = 1.9)),
  lower = list(continuous = wrap("points", alpha = 0.3, size=0.1))) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(title = "Pairwise relationship, colored by sex, between variables in the
       insurance dataset")
```



-----------


Add variable `smoker` to your model in R:

```{r}
mod_age_smk = lm(charges ~ age + smoker, data = ins)
mod_age_smk
```

```{r}
summary(mod_age_smk)
```

-----------

What is the actual mathematical model that R was fitting?

To answer this we need to understand what a dummy variable is.  


## **Dummy variable**

A *dummy variable* is a variable that gives only 0 and 1 values.  For example, if $1_{smoker, i}$ is a dummy variable for observation $i$ then $1_{smoker, i}$ has the value 1 if the $i^{th}$ object is a smoker and 0 if not.

----------


The mathematical model is

\begin{align}
charges_i = \beta_0 + \beta_{1_{smoker='yes', i}}1_{smoker='yes', i} + \beta_{age}age_i +  error_i
\end{align}

$\beta_{1_{smoker='yes', i}}$ is the effect on `charges` for being a smoker.

\begin{align}
width_i = \beta_0 + \beta_{1_{Species='versicolor', i}}1_{Species='versicolor', i} + \beta_{1_{Species='virginica', i}}1_{Species='virginica', i} +  error_i
\end{align}

----------


**Adding a categorical variable to the model results in including dummy variable(s) in the mathematical model.   The practical result is that we now have two lines for predicting `charges`, one for each group of the smokers and the non-smokers; each group can now have a line with different y-intercepts.**  


$\beta_{smoker=}$ is the additional effect on `charges` for being a smoker; Note that the non-smokers are the reference group (also called the baseline group).  By default `R` will always use the group corresponding to the first level of the factor as the reference group.

----------

Can you see why the model above actually gives two predicting lines, one for the non-smokers and one for the smokers?



# Adding an interaction term

----------

Can we have different slops?

----------

From the scatterplot matrix colored by `smoker` we see that it is actually not desirable for the two groups (smokers and non-smokers) to have two predicting lines with different slopes (why?).  However, just as a practice we will fit a model with different slopes.

----------


```{r}
mod_age_x_smk = lm(charges ~ age*smoker, data = ins)
summary(mod_age_x_smk)

# This model is the same as 
lm(charges ~ age + smoker + age:smoker, data = ins)

```



What is the mathematical model that corresponds to this?


Question: Which of the following is the mathematical model that correspond to the model in R `lm(charges ~ age + smoker + age:smoker, data = ins)`

(A.) \begin{align}
charges_i = \beta_{1_{smoker='yes', i}}1_{smoker='yes', i} + \beta_{age}age_i +  \beta_{age:smoker}age:1_{smoker='yes', i} + error_i
\end{align}
(B.) \begin{align}
charges_i = \beta_0 + \beta_{age}age_i +  \beta_{age:smoker}age:1_{smoker='yes', i} + error_i
\end{align}
(C.) \begin{align}
charges_i = \beta_0 + \beta_{1_{smoker='yes', i}}1_{smoker='yes', i} +  \beta_{age:smoker}age:1_{smoker='yes', i} + error_i
\end{align}
(D.) \begin{align}
charges_i = \beta_0 + \beta_{1_{smoker='yes', i}}1_{smoker='yes', i} + \beta_{age}age_i +  \beta_{age:smoker}age:1_{smoker='yes', i} + error_i
\end{align}


Answer: D

The term $age:1_{smoker='yes'}$ is called an interaction term; including this interaction term in our model we allow the variable `age` to have different effect in `charges` depending whether the person that we are interested in is a smoker or not.


# Predicting with a model 

## Giving a point estimate

To use the model in option D to predict the annual total medical expenses charged to the policy by the policy holder we will do the following:

Let 

* $\hat{\beta}_0 = -2091.42$  
* $\hat{\beta}_{age} = 267.25$  
* $\hat{\beta}_{smoker='yes'} = 22385.55$  
* $\hat{\beta}_{age:1_{smoker='yes'}} = 37.99$  

If the person is a **non-smoker** we will predict this person's annual total medical expenses to be:

\[
charges = \hat{\beta}_0 + \hat{\beta}_{age}age_i
\]

Thus, the **y-intercept** for this line is $\hat{\beta}_0$ and the **slope** is **$\hat{\beta}$**.

If the person is a **smoker** we will predict this person's annual total medical expenses to be:

\[
charges = \hat{\beta}_0 + \hat{\beta}_{smoker='yes'} + \hat{\beta}_{age}age +  \hat{\beta}_{smoker='yes'}age
\]

In this case the **y-intercept** for this line is **$\hat{\beta}_0 + \hat{\beta}_{smoker='yes'}$** and the **slope** is **$\hat{\beta} + \hat{\beta}_{smoker='yes'}$**.


Therefore, $\beta_{1_{smoker='yes', i}}$ is the additional effect on `charges` for being a smoker (compared to the non-smokers); $\beta_{age:smoker}$ is the additional effect on `charges` for each unit of increment in `age` (compared to the non-smokers).

For example, for a 43 year old non-smoker we will predict this person's annual medical amount by using the fitted model

\[
\hat{\beta}_0 + \hat{\beta}_{age}*43 = -2091.42 + 267.25(43)
\]

Thus, we will predict the person's annual medical amount to be **9400.33**.

```{r}
sum(mod_age_x_smk$coef * c(1, 43, 0, 0))
# remember not to hard code values
```


For a 43 year old smoker the prediction becomes


\[
(\hat{\beta}_0 + \hat{\beta}_{smoker='yes'}) + (\hat{\beta}_{age} +  \hat{\beta}_{smoker='yes'}) age = (-2091.42 + 22385.55) + (267.25 + 37.99)(43)
\]

this figure becomes **33419.35**.

```{r}
sum(mod_age_x_smk$coef * c(1, 43, 1, 43))
# remember not to hard code values
```

This is saying that with everything else being equal being a smoker will have charge $22385.55 more to their insurance plan on average. 



# Summary

Including a **factor with m levels** in the model will 

* induce **(m-1) dummy variables**, one for each level of the factor except for the first level, to the mathematical model;  

* allow the model to **a different y-intercept** for different levels for the prediction line.

Including an **interaction term** between a factor (with m levels) and a continuous variable in the model will 

* induce **(m-1) variables**, one for each level of the factor except for the first level, to the mathematical model;  

* allow the model to a **different slope** for different levels for the prediction line.

# Technical details related to concepts and functions covered in this chapter

--------

In general a regression model has the form

\[
y_i = (\beta_0 + \beta_1 x_{1,i} + \dots + \beta_k x_{k,i}) + error_i
\]

for $i  = 1, 2, ..., n$ where n is the sample size and $k$ is smaller than $n$ in general.

----------


## The error term

The error term is used to catch the deviations between an observation and the expected value of y for a given set of x values.  The distribution of the errors give you an idea of how much the y values vary around the regression line.  

The assumption on the errors is that they have mean zero for each given x-value.

## Interpretation on the points on the line

Because the errors have mean zero, 

\[
mean(Y) = \beta_0 + \beta_1 x_{1,i} + \dots + \beta_k x_{k,i}
\]

i.e., the point on line gives the mean of all the y-values that share the same x-value.

## Matrix representation of the model

\[
y_i = (\beta_0 + \beta_1 x_{1,i} + \dots + \beta_k x_{k,i}) + error_i
\]

can be written in matrix form as

$$
\begin{bmatrix}
y_1\\
y_2\\
y_3\\
\vdots\\
y_n
\end{bmatrix}
=
\begin{bmatrix}
1&x_{11}&x_{12}&\cdots &x_{1k} \\
1&x_{21}&x_{22}&\cdots &x_{2k} \\
\vdots&\vdots & \vdots & \ddots & \vdots\\
1&x_{n1}&x_{n2}&\cdots &x_{nk}
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1\\
\beta_2\\
\vdots\\
\beta_k
\end{bmatrix}
+
\begin{bmatrix}
error_1\\
error_2\\
error_3\\
\vdots\\
error_n
\end{bmatrix}
$$

