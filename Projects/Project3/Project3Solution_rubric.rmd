---
title: 'SML 201 Mini Project 3: 2016 U.S. Presidential Election'
author: ''
date: "`r Sys.Date()`"
output:
  pdf_document:
    dev: png
    fig_caption: yes
  html_document:
    df_print: paged
geometry: margin=1in
fontsize: 10pt
subtitle: Solutions
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.height=4.5, fig.width=8, collapse=T, comment="", prompt=F, echo = T, cache=T, autodep=T, tidy=T, tidy.opts=list(width.cutoff=63), dev='png')
options(width=63)

```

**Project 3 is due by 11:59pm on Monday November 25.**  Please submit both a .Rmd and a .pdf file on Blackboard by the deadline **and** drop off a hard copy of the pdf file at 26 Prospect Avenue by 5pm of the **next weekday** of the due date. To look for the drop-off cabinet, after you enter the building turn to the left to enter the lounge area and the file cabinet is to your right with an open slot with the label "SML 201 Homework"; note that the building might be locked after 6pm and on the weekends.  You are also welcome to bring your PDF copy to any lecture **before** the deadline and I will drop off the copy for you.

Late **projects** will be penalized at intervals rounded up to multiples of 24 hours.  For example, if you are 3 hours late, 10% off or if you are 30 hours late, 20% off.

This project can be completed in groups of up to 3 students. It is okay to work by yourself, if this is preferable.   You are welcome to get help (you can either ask questions on Piazza or talk to instructors in person during office hours) from instructors; however, please do not post code/solutions on Piazza on a public post.  

You are encouraged to get help from the instructors (either through Piazza or in person) if you need help to understand the definitions of the variables of the dataset or the procedure of the experiment.

When working in a group it is your responsibility to make sure that you are satisfied with all parts of the report and the submission is on time (e.g., we will not entertain arguments that deficiencies are the responsibility of other group members).  We expect that you each work independently first and then compare your answers with each other once you all finish, or you all work together on your own laptops.  Failing to make contributions and then putting your name on a report will be considered a violation of the honor code.  **Please do not divide work among group mates.**  Everyone in your group is responsible for being able to explain the solutions in the report.

For all parts of this problem set, you **MUST** use R commands to print the output as part of your R Markdown file. You are not permitted to find the answer in the R console and then copy and paste the answer into this document.

**If you are completing this project in a group**, please have only **one** person in your group turn in the .Rmd and .pdf files; the other person in your group should turn in the list of the people in your group
in the *Text Submission* field on the submission page.  This means that **everyone should make a submission**--either a file-upload or a text submission--regardless of whether you are working in a group or not.

**The physical pdf report that you drop off and the .pdf file that you submit on Blackboard should be identical.  Modifying your report after the deadline could result in a penalty as much as getting a zero score for the assignment.**


----

Please type your name(s) after "Digitally signed:" below the honor pledge to serve as digital signature(s).  Put the pledge and your signature(s) at the beginning of each document that you turn in.

> I pledge my honor that I have not violated the honor code when completing this assignment.

Digitally signed:

----


**In order to receive full credits, please have sensible titles and axis labels for all your graphs and adjust values for all the relevant graphical parameters so that your plots are informative.  Also, all answers must be written in complete sentences.**


## (-3 pts each if any of these are not satisfied: code runs, code has annotations, answers are in complete sentences)

## Before you start: loops are not allowed for this project.  Please report all numerical answers to 2 digits after the decimal.  Remember not to round intermediate calculations and please avoid hard-coding.

## Please submit a color copy for the physical pdf file.

-------



**Please read and work through all the parts in *Project 3 Hints*.  Doing so will help you avoid making some common mistakes for this project.  Also, you will need to have the `maps` package installed for this project.**  



# Objective of this project

The outcome of the 2016 presidential election was a big surprise to many of us; all 6 probabilistic and 3 non-probabilistic models published on New York Times missed the election result [(click here for reference)](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html).   Even on November 8, 2016 (the election day) before the voting results were out Clinton was still estimated to have a winning chance of 71.4% (compared to 28.6% for Trump) by Nate Silver, a statistician who had a track record of predicting the presidential election outcomes correctly until 2016 [(click here for reference)](https://en.wikipedia.org/wiki/Nate_Silver). 

Many believe that the main factor contributes toward the failure of these models was the bias in the polling data [(click here for reference)](http://www.pewresearch.org/fact-tank/2016/11/09/why-2016-election-polls-missed-their-mark/).  In this project we will explore the problems in the polling data that could potentially affect the accuracy of the presidential election prediction.

# Background info and datasets used in this project

We will use two datasets: `oct_poll.Rdata` and `2016_election_result_csv.csv` in this project.

The dataset `oct_poll.Rdata` is a subset of the dataset from this website (https://www.r-bloggers.com/fivethirtyeights-polling-data-for-the-us-presidential-election/).  `oct_poll.Rdata` contains poll data collected from polls that were taken between Oct. 1, 2016 and Oct. 27, 2016.  The original dataset was first posted on fivethirtyeight.com, a website created by Nate Silver.  (Note: Even though Silver's prediction also missed the truth, Nate Silver's model was the most robust one among all the models published on *New York Times* for predicting the 2016 presidential election result; a robust model is a model that performs well even when some of the assumptions of the model were slightly violated.) 



# How did all 6 probabilistic models published on New York Times missed the election result?

In this section we will study the patterns of the errors in the poll data.  We would like to answer these two questions: 

* Do the errors in the poll dataset seem random? Are they distributed symmetrically around zero?   
* Among the states with CI estimates that do not cover zero (i.e., among the states that we were confident about who the winning candidates for the states would be), how many of their CIs actually predicted the correct state winner?


## (18 pts) Question 1 

In 2016 the election day was Nov. 8.  `oct_poll.Rdata` contains poll data collected from polls that ended between Oct. 1, 2016 and Oct. 27, 2016; the dataset has 752 rows and 6 columns.  If you forget how to read in this dataset try running `?load` to get help. You do not need to supply the values for the `envir` and `verbose` arguments in `load()`.   

The descriptions of the column names are:

* **state**: The state which was polled;
* **grade**: The rating of the company taking the poll;
* **samplesize**: Sample size of the poll;
* **rawpoll_clinton**: The percentage of people favoring Clinton in the sample;
* **rawpoll_trump**: The percentage of people favoring Trump in the sample;
* **end.date**: The closing date of the poll.

Note that there are multiple entries for each state since there are multiple poll agencies for each state.  Also note that on each row the percentages for Clinton and Trump do not add up to 100\% since there are other candidates.  


#### Answer
### (1 pt to load data set)

```{r}
library(reshape2)
library(plyr)
library(ggplot2)

# Load polling data
load("oct_poll.Rdata")
dim(oct_poll)
str(oct_poll)

```

### (17 pts) Part a


Make a data frame `poll.oct2016`(see examples on how to create a data frame in Chapter 2.3) ; this data frame should have 4 columns:

* `state`
* `samplesize`
* `rawpoll_clinton`
* `rawpoll_trump`.  

`poll.oct2016` should aggregate the data in `oct_poll` to the state level (we want to combine multiple polls within the same state into one sample so that we have a sample that is as large as possible.) and express `rawpoll_clinton` and `rawpoll_trump` as proportions instead of percentages.  You should keep `District of Columbia` even though technically it is not a state. `poll.oct2016` should have one row for each state.  You need to be careful about how to aggregate the percentages; e.g., suppose state 1 has two polls: poll 1 has a sample of 100 and 30\% of the people in the sample voted for Clinton and poll 2 has a sample of 400 and 60% of the people in the sample voted for Clinton; then, the aggregated proportion of people voted for Clinton in the polls for state 1 should be (100 x .3 + 400x.6)/(100+400) = .54.  

Hint: If you do not remember how to do this, it is time to review what object type the `tapply()` outputs are of; also remember that `as.vector()` removes element names of the input object by default; thus, you will need to assign the element names back to the vector after you convert a R object into a vector by using `as.vector()`; see Chapter 2.3 for a review on both of these two concepts.

#### Answer

```{r}
# Generate a function to aggregate the polls
agg.samp <- function(x, ind = oct_poll$state){
                      agg = tapply(x, INDEX=ind, FUN=sum)
                      states = names(agg)
                      agg2 = as.vector(agg)
                      names(agg2) = states
                      return(agg2)
}

# Aggregate the polling data
clinton.poll <- agg.samp(oct_poll$samplesize*oct_poll$rawpoll_clinton/100)
trump.poll <- agg.samp(oct_poll$samplesize*oct_poll$rawpoll_trump/100)
samplesize <- agg.samp(oct_poll$samplesize)

# Create requested dataset
poll.oct2016 <- data.frame(state = names(samplesize), 
                          samplesize = samplesize, 
                          rawpoll_clinton = clinton.poll/samplesize, 
                          rawpoll_trump = trump.poll/samplesize)

```

```{r}
# check to see if the columns have the correct data type
str(poll.oct2016)
```


## (16 pts) Question 2

### (10 pts) Part a

'2016_election_result_csv.csv' contains the presidential election results for 2016.  Read in the dataset `2016_election_result_csv.csv` and name this dataset `true.perc`. Use the `merge()` function in R to merge `true.perc` with `poll.oct2016` by using the state names (you can review how to use the `merge()` function in Chapter 2.3).  In addition, add the following columns to the data frame after the merging (see Chapter 2.3 again for how to use the `mutate()` function):

* **poll.diff** = rawpoll_trump - rawpoll_clinton
* **vote.diff** = trump.vote - clinton.vote
* **deviation** = poll.diff - vote.diff

Name the resulting data frame `election`.  `election` should have 51 rows and 9 columns.

#### Answer

```{r}
# Read in election data
true.perc <- read.csv(file='2016_election_result_csv.csv')
str(true.perc)

# merge datasets and add new vars
election <- merge(poll.oct2016, true.perc, by="state") 
election <- mutate(election, 
                  poll.diff = rawpoll_trump - rawpoll_clinton, 
                  vote.diff = trump.vote - clinton.vote, 
                  deviation = poll.diff - vote.diff)
```

```{r}
# check results
str(election)
dim(election)
```

### (6 pts) Part b 

Choose the correct answer for each of the following questions:

 

(2 pt) (i.)  For a particular state if `poll.diff` is positive this means  

(A.) for the people in the *combined poll* for the state there are more people 
supporting Trump than supporting Clinton;  
(B.) for the people in the *combined poll* for the state there are less people 
supporting Trump than supporting Clinton;  
(C.) for the people who casted their votes in the *actual election* there are 
more voters supporting Trump than supporting Clinton within the state;      
(D.) for the people who casted their votes in the *actual election* there are 
less voters supporting Trump than supporting Clinton within the state;  
(E.) Trump won the actual election at the state level;  
(F.) Clinton won the actual election at the state level.

Answer: A.  


(4 pt) (ii.)  For a particular state if `deviation` is positive this means

(A.) Trump won the actual election at the state level;  
(B.) Trump lost the actual election at the state level;  
(C.) at the state level the poll over-estimated how well Trump would do in the actual election 
compared to Clinton;  
(D.) at the state level the poll under-estimated how well Trump would do in the actual election 
compared to Clinton.    

Answer: C.

## (66 pts) Question 3

### (23 pts) Part a

Create the following vectors and add them as the columns to `election`:

* **std.unit**: For the $i^{th}$ state `std.unit[i]` is `poll.diff[i]` in standard units assuming `vote.diff[i]` is the mean for `poll.diff[i]`; e.g., if `poll.diff[1]` is 3 SE(poll.diff for state i) above `vote.diff[i]`, `std.unit[i]` should be 3; if `poll.diff[1]` is 2.5 SE(poll.diff for state i) below `vote.diff[1]`, `std.unit[1]` should be -2.5.  (See exercise 1.h in Precept 9.)

* **same.sign**: A logical vector whose $i^{th}$ element should be `TRUE` if both `poll.diff[i]` and `vote.diff[i]` have the same sign (i.e., either both positive or both negative), and `FALSE` otherwise.  

* **significant**: A logical vector that shows `TRUE` if the 95% CI for estimating (proportion of Trump supporters - proportion of Clinton supporters) does not cover 0 and `FALSE` otherwise.

#### Answer

Use the approach that we discussed in the hints:

```{r}
# Create function to determine std.unit
t.stat = function(x){
  # Get the sample
  sampl = rep(c(-1, 0, 1), 
            times = round(c(x['rawpoll_clinton']*x['samplesize'], 
                            (1-x['rawpoll_clinton']-x['rawpoll_trump'])*x['samplesize'], 
                            x['rawpoll_trump']*x['samplesize']) 
                          )
            )  
  # Pull out std. unit and p.v.
  output = c(t.test(sampl, mu=x['vote.diff'])$statistic, 
             t.test(sampl, mu=0)$p.value,
 # Check whether the CI covers 0 directly
 # If the CI doesn't cover 0 the product of the two end points
 # should be positive
             t.test(sampl)$conf.int[1]*t.test(sampl)$conf.int[2]
             )
  
  

  names(output) = c("std.unit", "p.v", "end.pt.product")                
  return(output)
}

head(election)

# Create input matrix and generate new variables:
tmp = as.matrix(election[,-1])
stat.p = apply(tmp, MAR=1, FUN = t.stat)
dim(stat.p)
class(stat.p[1,])

# Create same.sign
same.sign = (election$poll.diff*election$vote.diff>0)
sum(same.sign)

# Create significant
# If the CI doesn't cover 0 the product of the two end points
# should be positive
significant = (stat.p[3,] > 0)

# or equivalently, the p-value for the two-sided test
# where the null is that the population mean is 0
# will have p-value less than 5%
# significant = (stat.p[2,] < .05)

sum(significant)

# Add new variables to election
tmp = data.frame(election, std.unit = stat.p[1,], same.sign = same.sign, significant = significant)
dim(tmp)

election = tmp
```

### (8 pts) Part b

Use the information that you calculated in 3.a to answer the following questions:

i. How many of the 95% CIs for estimating (proportion of Trump supporters - proportion of Clinton supporters) mentioned in Part (a) are away from 0?  
ii. Among the CIs that are away from 0 how many of them predict the winning candidate of the state correctly?  
iii. How many of the 95% CIs cover 0?  
iv. Among the states whose CIs cover 0 how many of them predict the winning candidate of the state correctly with their `poll.diff` value?



#### Answer

```{r}
signif.sign = table(election$significant,election$same.sign)
rownames(signif.sign) = c('Not significant', 'significant')
colnames(signif.sign) = c('Different signs', 'Same signs')
signif.sign
```

i. Forty-eight (48) of the 95% CIs mentioned in Part (a) are away from 0.  
ii. Among the 48 CIs that are significant (i.e., away from 0), 43 of them predict the winner of the state correctly  
iii. Three (3) of the 95% CIs cover 0.  
iv. Among the CIs that cover 0, 2 of them predict the winning candidate of the state correctly.

### (12 pts) Part c

The error for a prediction is defined as 

> Prediction error := Predicted value - Actual value

The errors in the polls are actually not much bigger than that in previous elections on average [(See NY Times article here)](https://www.nytimes.com/2016/11/23/upshot/election-facts-to-keep-handy-for-thanksgiving-dinner-discussion.html?_r=0).  However, the problem is that the polls that over estimated Clinton's votes are polls for the battleground states.  


**(6 pts) i.**  In Part b.ii you reported the number of CIs that correctly predict the winning candidate of the state among the CIs that are away from 0.   Among the CIs that are away from 0, how many incorrectly predicts the winner of the state?   List the names of the states that associate with these CIs; for these states did the polls predict that Clinton was going to win or lose the election for the state? 

#### Answer

```{r}
# Get state names of incorrect polls
flip.state <- election$state[election$significant & (!election$same.sign)]
flip.state

# Check to see the vote.diff values for the flip.state
election$vote.diff[election$state %in% flip.state]
# compare with poll.diff
election$poll.diff[election$state %in% flip.state]
# They all over-estimate how Clinton would do

```

Answer:  No states incorrectly predicted a Trump victory.  Five (5) states incorrectly predicted a Clinton victory.  These states were Florida, Michigan, North Carolina, Pennsylvania, and Wisconsin.

**(3 pts) ii.** According to 
Wikipedia [(click here)](https://en.wikipedia.org/wiki/Swing_state) 

>Election analytics website FiveThirtyEight identifies the states of Colorado, Florida, Iowa, Michigan, Minnesota, Ohio, Nevada, New Hampshire, North Carolina, Pennsylvania, Virginia, and Wisconsin as 'perennial' swing states that have regularly seen close contests over the last few presidential campaigns.  

Which of the states that you identified in part i are among the swing states? 

```{r}
swing = c("Colorado", 'Florida', 'Iowa', 'Michigan', 'Minnesota', 'Ohio', 'Nevada', 'New Hampshire', 'North Carolina', 'Pennsylvania', 'Virginia', 'Wisconsin')

length(swing)

flip.state

flip.state %in% swing
```

#### Answer

All five states identified in part i are among the swing states; i.e., all the states that the polls predicted wrong are swing states.  The use of `%in%` is not required for this problem.



**(3 pts) iii**. We are going to plot the values of `std.unit` on the map to investigate the geographic pattern of the `std.unit` values.  Please make sure that you have installed two packages: `maps` and `dplyr`. 

Run the code in the following code chunk will create the function `create.map()`.  `create.map()` will make a map for the states defined by the variable `input.state`, color the states on the map according to the values in `map.var`, and then outlines the states specified by the variable `outlined.state`.  (You do not need to understand line by line how the function is being constructed; you just need to know how to use the function.)


```{r echo =F}
create.map = function(input.state, map.var, outlined.state){

# Load package needed     
library(ggplot2) 
library(maps)
library(dplyr)

# Remove axes for the map  
drop_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )

# The map should be at the state level
states = map_data("state")  

# Make data frame for the data for the map
map.data = data.frame(state = input.state, map.var=map.var)
names(map.data)
names(map.data)[1] = "region"
map.data$region = tolower(as.character(levels(map.data$region))) 

state.map.data = inner_join(states, map.data, by = "region")

# Make data for the states to be outlined on the map
swing.outline = map_data("state", region = outlined.state)

ggplot(data = state.map.data, 
       mapping = aes(x = long, y = lat, group = group)) + 
      geom_polygon(data = state.map.data, aes(fill = map.var), color = "gray") +
      scale_fill_gradient2(low = "#CF142B", mid = "white", 
                           high = "#006600", midpoint = 0) +
      geom_path(data = swing.outline, aes(x = long, y = lat, group = group), size=.7) +
    #  ggtitle('( where diff = Trump % -Clinton %') + 
      
      labs(fill = "std.unit", 
           title = "Values for std.unit = (poll.diff - actual.diff)/SE(poll.diff)", 
           subtitle = 'where diff = Trump % -Clinton %') +
      coord_fixed(1.4) +
     # scale_fill_continuous(labels = c("poll.diff")) + 
      theme_bw() +
      drop_the_axes
}

```

Here is an example of how to use the function (you will need to remove `eval = F` if you would like to show this in your report).  The following code draws the states Michigan and North Carolina, colors Michigan with the color that correspond to the value 1 and colors North Carolina with the color that correspond to the value 2.  The code also outlines Michigan.

```{r eval = F}
create.map(input.state = c("Michigan", "North Carolina"), 
map.var = c(1,2), outlined.state = c("Michigan"))
```

Use `create.map()` to plot the values of `std.unit` for each state and then outline the swing states on the map.  (Note that Alaska, Washington D.C. and Hawaii will not show up on the map because the package used to draw the map excludes these places.)

Hint: `input.state` should be a factor or a character vector of the state names, `map.var` should be a numeric vector and `outlined.state` should be a character vector of the names of the states you would like to outline.  Please make sure the `input.state` and `map.var` have the same length and the elements in both vectors are arranged in the same order.

Based on the map do the polls under- or over-estimated Clinton's chance of winning for most of the swing states?

#### Answer


```{r}
create.map(input.state = election$state, map.var = election$std.unit, outlined.state = swing)
```

Answer: Based on the map the polls over-estimated Clinton's chance of winning for most of the swing states.


### (5 pts) Part d

Fill in the value for the x argument below to make a histogram for the values of `std.unit` and mark the location where `std.unit = 0` with a red vertical line.  Do the errors in the estimates look like they have mean zero?


```{r eval = F}
# Generate histogram
hist(x = ..., breaks=25, freq = F, main = 'Poll Difference Errors in Standard Units', xlab = 'Poll Diff. in Std. Units Relative to the Truth', cex.main=.8)
abline(v=0, col='red')
text(x= 0, y = 0, label = 'std.unit = 0', col = 'red')

```


#### Answer


```{r}
# Generate histogram
hist(election$std.unit, breaks=25, freq = F, main = 'Poll Difference Errors in Standard Units', xlab = 'Poll Diff. in Std. Units Relative to the Truth', cex.main=.8)
abline(v=0, col='red')
text(x= 0, y = 0, label = 'std.unit = 0', col = 'red')
```

The errors seem to primarily fall well below zero, suggesting a polling bias in favor of Clinton.

### (8 pts) Part e

How many states have polls underestimated (i.e., the value in `std.unit` is less than zero) how well Trump would do? Let's call this number $m$.

If the errors in the poll data were random, there should be equally likely chance for a state to underestimate or overestimate Trump.  Then, the values in `std.unit` should be like 51 values drawn from a common distribution independently.

What is this common distribution?  Report the numerical value(s) of the parameters of this distribution.  According to this common distribution what is the chance $g$ that the `std.unit` value for a particular state would be positive?

#### Answer

```{r}
# Count the number of errors below zero
table(election$std.unit < 0)
```

The common distribution is Normal(0, 1).  According to Normal(0, 1) the chance of getting a positive value for `std.unit` is 0.5.


### (10 pts) Part f

If the errors in the poll data were random and independent among the states, each state should have $g$ probability to underestimate how well Trump would do.  Calculate the chance that we would have $m$ or more states that underestimate how well Trump would do in this case.

Hint: If the errors in the poll data were random and independent among the states, each state should have $g$ probability of underestimating how well Trump would do.  You can think of the model as having a large population with $g$ fraction of 1's and $1-g$ fraction of 0's.  Here the question is asking that if you randomly select 51 numbers with replacement from this population, what is the chance that you would observe $m$ or more 1's?



Based on the data is it reasonable to assume that the errors in the poll data were random and independent among the states?  Explain why.


#### Answer



There are 43 states that underestimated Trump, compared to 8 that underestimated Clinton.



Since the sample size is large (51) and p is not too small, we can also use the Normal approximation for calculating the p-value

```{r}
# number of positive std.unit values:
pos.no = sum(election$std.unit < 0)

sampl = rep(0:1, c(nrow(election) - pos.no, pos.no))

n.size = length(sampl)

se = sqrt((.5)*(1-.5)/n.size)

1 - pnorm((mean(sampl) - .5)/se)

```



The chance is around $4.7680 \times 10^{-7}$.  If the errors in the poll data were random and independent among the states, the chance that we would observe 43 states underestimating how well Trump would do is almost zero.  However, we do observe this many states in our data.  This is strong evidence against the assumption that the errors in the poll data were random and independent among the states.  






